name: NVIDIA Model Benchmark Job
on:
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Unique identifier for this run'
        required: true
        type: string
      payload:
        description: 'Content of the user submission config, as compressed json string'
        required: true
        type: string

run-name: 'Model Job - ${{ github.event.inputs.run_id }}'

jobs:
  run:
    runs-on: [nvidia-docker-b200-8-x86-64]
    timeout-minutes: 60
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      GITHUB_REPOSITORY: ${{ github.repository }}
      HF_TOKEN: ${{ secrets.HF_TOKEN }}
      CUDA_VISIBLE_DEVICES: "4,5,6,7"
    steps:
    - uses: actions/checkout@v3

    - name: Create input files
      shell: bash
      run: |
        # Extract the payload content without printing it
        apt-get update && apt-get install -y jq
        PAYLOAD=$(jq -r '.inputs.payload' $GITHUB_EVENT_PATH)

        # Apply mask to the extracted content
        echo "::add-mask::$PAYLOAD"

        # Now write to file (won't be logged since it's masked)
        echo "$PAYLOAD" > payload.json

    - name: Install uv
      uses: astral-sh/setup-uv@v4

    - name: Setup environment
      shell: bash
      run: |
        uv venv .venv --python 3.10
        echo "VIRTUAL_ENV=$PWD/.venv" >> $GITHUB_ENV
        echo "$PWD/.venv/bin" >> $GITHUB_PATH

        # Install torch with CUDA 12.8 â€” the vLLM pip wheel is compiled
        # against CUDA 12, so we need cu128 torch for libcudart.so.12.
        uv pip install torch==2.9.1 --index-url https://download.pytorch.org/whl/cu128

        # Keep vLLM installed so the benchmark runner can use the fast overlay
        # path: user's Python files are copied on top of the installed package
        # (~instant) instead of a full pip install from source (~20 min).
        uv pip install vllm

        # Install kernelbot
        uv pip install -r "requirements-dev.txt"
        uv pip install -e .

    - name: Run model benchmark
      shell: bash
      env:
        SETUPTOOLS_SCM_PRETEND_VERSION: "0.0.1.dev0"
      run: |
        python3 src/runners/github-runner.py

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: run-result
        path: result.json
